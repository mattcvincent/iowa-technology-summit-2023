# Question #15

Now help me test-drive making a `util/lake_loader.py` file that calls `ApiClient`'s `fetch_data` method and feeds the output of that into  `DataTransform`'s `flatten_data` method and the output of that into `save_to_parquet` (use the filename of `flattened_timetrack_data.parquet`).

---

# Answer #15

Sure! Here's an example test-driven implementation of `util/lake_loader.py` that uses `ApiClient` and `DataTransform` classes to fetch, transform, and save data:

```python
from app.api import ApiClient
from app.transform import DataTransform
from dotenv import load_dotenv

def main():
    # Load environment variables
    load_dotenv()
    
    # Initialize objects
    api_client = ApiClient()
    data_transform = DataTransform()

    # Fetch data
    raw_data = api_client.fetch_data(dateFrom='2022-01-01')

    # Transform data
    flattened_df = data_transform.flatten_data(raw_data)

    # Save data
    data_transform.save_to_parquet(flattened_df, "flattened_timetrack_data.parquet")

if __name__ == '__main__':
    main()
```


And here's a test to verify that `util/lake_loader.py` works as expected:

```python
import os
import unittest
from unittest.mock import patch
from util import lake_loader
import pandas as pd
from pandas.testing import assert_frame_equal
from app.api import ApiClient

class TestLakeLoader(unittest.TestCase):
    
    @patch.object(ApiClient, 'fetch_data')
    def test_main(self, mock_fetch_data):
        # Define test data
        input_data = [
            {
                "userId": 1,
                "records": [
                    {
                        "taskId": 116,
                        "time": 120
                    },
                    {
                        "taskId": 135,
                        "time": 105
                    }
                ],
                "dayOffset": 0,
                "date": "2023-03-01"
            },
            {
                "userId": 15,
                "records": [
                    {
                        "taskId": 116,
                        "time": 90
                    },
                    {
                        "taskId": 117,
                        "time": 150
                    },
                    {
                        "taskId": 118,
                        "time": 165
                    }
                ],
                "dayOffset": 0,
                "date": "2023-03-01"
            }
        ]
        mock_fetch_data.return_value = input_data

        # Call main() to process test data
        lake_loader.main()

        # Load saved data from file
        output_file = "flattened_timetrack_data.parquet"
        self.assertTrue(os.path.isfile(output_file))
        actual_output = pd.read_parquet(output_file)

        # Define expected output
        expected_output = {
            "userId": [1, 1, 15, 15, 15],
            "taskId": [116, 135, 116, 117, 118],
            "time": [120, 105, 90, 150, 165],
            "dayOffset": [0, 0, 0, 0, 0],
            "date": ["2023-03-01", "2023-03-01", "2023-03-01", "2023-03-01", "2023-03-01"]
        }

        # Verify output data matches expected data
        assert_frame_equal(actual_output[expected_output.keys()], pd.DataFrame(expected_output), check_dtype=False)
```

Note that in the test, we are mocking the `fetch_data` method of `ApiClient` to return our test data instead of calling the actual API. This allows us to test the data transformation and storage components of `util/lake_loader.py` in isolation. We are also using `assert_frame_equal` from the `pandas.testing` module to compare the saved data to our expected output.


---

# Action #15

Create `lake_loader.py` and `test_lake_loader.py`:

```bash
mkdir util/

cat <<EOT > util/lake_loader.py
from app.api import ApiClient
from app.transform import DataTransform
from dotenv import load_dotenv

def main():
    # Load environment variables
    load_dotenv()
    
    # Initialize objects
    api_client = ApiClient()
    data_transform = DataTransform()

    # Fetch data
    raw_data = api_client.fetch_data(dateFrom='2022-01-01')

    # Transform data
    flattened_df = data_transform.flatten_data(raw_data)

    # Save data
    data_transform.save_to_parquet(flattened_df, "flattened_timetrack_data.parquet")

if __name__ == '__main__':
    main()
EOT

cat <<EOT > tests/test_lake_loader.py
import os
import unittest
from unittest.mock import patch
from util import lake_loader
import pandas as pd
from pandas.testing import assert_frame_equal
from app.api import ApiClient

class TestLakeLoader(unittest.TestCase):
    
    @patch.object(ApiClient, 'fetch_data')
    def test_main(self, mock_fetch_data):
        # Define test data
        input_data = [
            {
                "userId": 1,
                "records": [
                    {
                        "taskId": 116,
                        "time": 120
                    },
                    {
                        "taskId": 135,
                        "time": 105
                    }
                ],
                "dayOffset": 0,
                "date": "2023-03-01"
            },
            {
                "userId": 15,
                "records": [
                    {
                        "taskId": 116,
                        "time": 90
                    },
                    {
                        "taskId": 117,
                        "time": 150
                    },
                    {
                        "taskId": 118,
                        "time": 165
                    }
                ],
                "dayOffset": 0,
                "date": "2023-03-01"
            }
        ]
        mock_fetch_data.return_value = input_data

        # Call main() to process test data
        lake_loader.main()

        # Load saved data from file
        output_file = "flattened_timetrack_data.parquet"
        self.assertTrue(os.path.isfile(output_file))
        actual_output = pd.read_parquet(output_file)

        # Define expected output
        expected_output = {
            "userId": [1, 1, 15, 15, 15],
            "taskId": [116, 135, 116, 117, 118],
            "time": [120, 105, 90, 150, 165],
            "dayOffset": [0, 0, 0, 0, 0],
            "date": ["2023-03-01", "2023-03-01", "2023-03-01", "2023-03-01", "2023-03-01"]
        }

        # Verify output data matches expected data
        assert_frame_equal(actual_output[expected_output.keys()], pd.DataFrame(expected_output), check_dtype=False)
EOT

```

Run the unit test:

```bash
python -m unittest discover -s tests -p "test_lake_loader.py"
```

Re-generate the coverage report:

```bash
coverage run -m unittest discover -s tests && \
coverage report && \
coverage html && \
open htmlcov/index.html
```

---

# Next

[Chat 16](16.md)