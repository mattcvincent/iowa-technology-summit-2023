# Question #16

 Now that the S3 bucket is created, let's create a third workflow to:

1. assume the AWS role just like in the last workflow
2. install the AWS command line interface (CLI)
3. grab the three environment variables (just like in the prior workflow's 'Set environment variables' step)
4. run `util/lake_loader.py` via `python -m util.lake_loader`
5. get the exported bucket name from the stack 'ai-assisted-private-bucket'
6. Copy the `project/flattened_timetrack_data.parquet` file into the s3 bucket


---

# Answer #16

Sure, here's a possible implementation for the workflow:


```yaml
name: Process and Upload Data to S3
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
permissions:
  id-token: write
  contents: write
  pull-requests: write
jobs:
  process_and_upload_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          role-to-assume: '${{ secrets.AWS_ROLE }}'
          aws-region: us-east-2
          role-session-name: GitHub-OIDC
          role-duration-seconds: 1200

      - name: Set environment variables
        env:
          ACTITIME_URL: ${{ secrets.ACTITIME_URL }}
          ACTITIME_USERNAME: ${{ secrets.ACTITIME_USERNAME }}
          ACTITIME_PASSWORD: ${{ secrets.ACTITIME_PASSWORD }}
        run: |
          echo "ACTITIME_URL=${ACTITIME_URL}" >> $GITHUB_ENV
          echo "ACTITIME_USERNAME=${ACTITIME_USERNAME}" >> $GITHUB_ENV
          echo "ACTITIME_PASSWORD=${ACTITIME_PASSWORD}" >> $GITHUB_ENV

      - name: Install dependencies
        working-directory: ./project
        run: |
          python -m pip install -r requirements.txt

      - name: Get bucket name
        run: |
          bucket_name=$(aws cloudformation describe-stacks --stack-name ai-assisted-private-bucket --query "Stacks[0].Outputs[?OutputKey=='BucketName'].OutputValue" --output text --region=us-east-2)
          echo "DATA_BUCKET_NAME=$bucket_name" >> $GITHUB_ENV

      - name: Create Parquet file
        working-directory: ./project
        run: >
          python -m util.lake_loader

      - name: Load Parquet file into S3 bucket
        working-directory: ./project
        run: >
          aws s3 cp flattened_timetrack_data.parquet s3://${{env.DATA_BUCKET_NAME}}/flattened_timetrack_data.parquet

```

This workflow assumes that the S3 bucket has already been created by the previous workflow (as named `ai-assisted-private-bucket`), and that its name has been exported as `AIAssistedDataBucketName`. It also assumes that you have set up the necessary environment variables for the AWS CLI (e.g. `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`), and that you have a `requirements.txt` file listing the dependencies required by `util/lake_loader.py`.


---

# Action #16

Add the new GitHub Action Workflow:

```bash
cd ../

cat <<EOT > .github/workflows/lake-loader.yml
name: Process and Upload Data to S3

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  id-token: write
  contents: write
  pull-requests: write
jobs:
  process_and_upload_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          role-to-assume: '\${{ secrets.AWS_ROLE }}'
          aws-region: us-east-2
          role-session-name: GitHub-OIDC
          role-duration-seconds: 1200

      - name: Set environment variables
        env:
          ACTITIME_URL: \${{ secrets.ACTITIME_URL }}
          ACTITIME_USERNAME: \${{ secrets.ACTITIME_USERNAME }}
          ACTITIME_PASSWORD: \${{ secrets.ACTITIME_PASSWORD }}
        run: |
          echo "ACTITIME_URL=\${ACTITIME_URL}" >> \$GITHUB_ENV
          echo "ACTITIME_USERNAME=\${ACTITIME_USERNAME}" >> \$GITHUB_ENV
          echo "ACTITIME_PASSWORD=\${ACTITIME_PASSWORD}" >> \$GITHUB_ENV

      - name: Install dependencies
        working-directory: ./project
        run: |
          python -m pip install -r requirements.txt

      - name: Get bucket name
        run: |
          bucket_name=\$(aws cloudformation describe-stacks --stack-name ai-assisted-private-bucket --query "Stacks[0].Outputs[?OutputKey=='BucketName'].OutputValue" --output text --region=us-east-2)
          echo "DATA_BUCKET_NAME=\$bucket_name" >> \$GITHUB_ENV

      - name: Create Parquet file
        working-directory: ./project
        run: |
          python -m util.lake_loader

      - name: Load Parquet file into S3 bucket
        working-directory: ./project
        run: |
          aws s3 cp flattened_timetrack_data.parquet s3://\${{env.DATA_BUCKET_NAME}}/flattened_timetrack_data.parquet
EOT


cd project/
```

Push to GitHub, then visit:

1. [GitHub Actions](https://github.com/mattcvincent/ai-assisted/actions/)
2. [AWS Management Console](https://us-east-2.console.aws.amazon.com/s3/buckets?region=us-east-2)

---

# Next

We're done :)