name: Process and Upload Data to S3

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  id-token: write
  contents: write
  pull-requests: write
jobs:
  process_and_upload_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          role-to-assume: '${{ secrets.AWS_ROLE }}'
          aws-region: us-east-2
          role-session-name: GitHub-OIDC
          role-duration-seconds: 1200

      - name: Set environment variables
        env:
          ACTITIME_URL: ${{ secrets.ACTITIME_URL }}
          ACTITIME_USERNAME: ${{ secrets.ACTITIME_USERNAME }}
          ACTITIME_PASSWORD: ${{ secrets.ACTITIME_PASSWORD }}
        run: |
          echo "ACTITIME_URL=${ACTITIME_URL}" >> $GITHUB_ENV
          echo "ACTITIME_USERNAME=${ACTITIME_USERNAME}" >> $GITHUB_ENV
          echo "ACTITIME_PASSWORD=${ACTITIME_PASSWORD}" >> $GITHUB_ENV

      - name: Install dependencies
        working-directory: ./project
        run: |
          python -m pip install -r requirements.txt

      - name: Get bucket name
        run: |
          bucket_name=$(aws cloudformation describe-stacks --stack-name iowa-technology-summit-2023-private-bucket --query "Stacks[0].Outputs[?OutputKey=='BucketName'].OutputValue" --output text --region=us-east-2)
          echo "DATA_BUCKET_NAME=$bucket_name" >> $GITHUB_ENV

      - name: Create Parquet file
        working-directory: ./project
        run: |
          python -m util.lake_loader

      - name: Load Parquet file into S3 bucket
        working-directory: ./project
        run: |
          aws s3 cp flattened_timetrack_data.parquet s3://${{env.DATA_BUCKET_NAME}}/flattened_timetrack_data.parquet
